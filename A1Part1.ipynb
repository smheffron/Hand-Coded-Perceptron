{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error passed on epoch  503\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Printing final outputs compared to expected outputs for each test data-----\n",
      "\n",
      "Test data  0: produced: [0.99829653]   [-0.00163857] expected 1   0\n",
      "Test data  1: produced: [-0.00469118]   [0.99069539] expected 0   1\n",
      "Test data  2: produced: [-0.00588296]   [0.99471798] expected 0   1\n",
      "Test data  3: produced: [1.01634861]   [0.00425382] expected 1   0\n",
      "Test data  4: produced: [0.00735603]   [1.00587325] expected 0   1\n",
      "Test data  5: produced: [1.02239244]   [0.00763968] expected 1   0\n",
      "Test data  6: produced: [1.01718404]   [0.02341774] expected 1   0\n",
      "Test data  7: produced: [0.9975934]   [-0.00417307] expected 1   0\n",
      "\n",
      "-----Printing final weights and bias - bias is the last term in each array-----\n",
      "\n",
      "Weights and bias for neuron 1 hidden layer 1:\n",
      " [[ 1.06319877]\n",
      " [ 1.02353675]\n",
      " [ 1.13419534]\n",
      " [-1.25484964]]\n",
      "Weights and bias for neuron 2 hidden layer 1:\n",
      " [[ 0.85400759]\n",
      " [ 0.9107342 ]\n",
      " [ 0.94501433]\n",
      " [-0.70808169]]\n",
      "Weights and bias for neuron 3 hidden layer 1:\n",
      " [[0.38828396]\n",
      " [0.34077639]\n",
      " [0.11772932]\n",
      " [0.35985192]]\n",
      "Weights and bias for neuron 1 hidden layer 2:\n",
      " [[0.47424018]\n",
      " [0.25289877]\n",
      " [0.01962371]\n",
      " [0.02735152]]\n",
      "Weights and bias for neuron 2 hidden layer 2:\n",
      " [[-0.18659287]\n",
      " [-0.30094572]\n",
      " [ 0.04679365]\n",
      " [ 0.06176212]]\n",
      "Weights and bias for neuron 3 hidden layer 2:\n",
      " [[0.87310119]\n",
      " [0.73138821]\n",
      " [0.27616945]\n",
      " [0.36660846]]\n",
      "Weights and bias for neuron 4 hidden layer 2:\n",
      " [[ 1.91884384]\n",
      " [ 1.31147425]\n",
      " [ 0.09014336]\n",
      " [-1.11077903]]\n",
      "Weights and bias for neuron 1 output layer:\n",
      " [[ 0.02514035]\n",
      " [ 1.1783085 ]\n",
      " [-0.74419944]\n",
      " [ 1.03724468]\n",
      " [ 1.11647222]]\n",
      "Weights and bias for neuron 2 output layer:\n",
      " [[ 0.95131904]\n",
      " [ 0.65008534]\n",
      " [ 0.8634636 ]\n",
      " [-0.99030419]\n",
      " [-0.25583109]]\n"
     ]
    }
   ],
   "source": [
    "###### %matplotlib inline\n",
    "\n",
    "# libs\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "\n",
    "# # nonlinear, with built in derive param if need diriv.    \n",
    "def tanh(x, derive=False):\n",
    "    if derive:\n",
    "        return (1.0 - x*x)\n",
    "    return ( (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)) )\n",
    "\n",
    "def sigmoid(x, derive=False):\n",
    "    if derive:\n",
    "        return x * (1.0 - x) \n",
    "\n",
    "    if (x < 0):\n",
    "        a = np.exp(x) \n",
    "        return (a / (1 + a))\n",
    "        \n",
    "    else:\n",
    "        return ( 1.0 / (1.0 + np.exp(-x)) )\n",
    "\n",
    "# data set (inputs)\n",
    "# I added 1 as the last input param to account for the bias\n",
    "X = np.array([\n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0, 1.0],\n",
    "    [0.0, 1.0, 0.0, 1.0],\n",
    "    [0.0, 1.0, 1.0, 1.0],\n",
    "    [1.0, 0.0, 0.0, 1.0],\n",
    "    [1.0, 0.0, 1.0, 1.0],\n",
    "    [1.0, 1.0, 0.0, 1.0],\n",
    "    [1.0, 1.0, 1.0, 1.0]\n",
    "]) \n",
    "\n",
    "# labels (desired outputs)\n",
    "y = np.array([[1,0], \n",
    "              [0,1], \n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,0],\n",
    "              [1,0]\n",
    "])\n",
    "\n",
    "\n",
    "# weights and biases\n",
    "# the bias is built in to each neuron's weight vector as the last value \n",
    "# (bias should always be dot with 1, effectively just adding)\n",
    "\n",
    "#hidden layer 1\n",
    "n1_w = np.array([[0.1], [0.2], [0.3], [0.2]])\n",
    "n2_w = np.array([[0.1], [0.1], [0.1], [0.1]])\n",
    "n3_w = np.array([[0.3], [0.3], [0.3], [0.9]])\n",
    "                 \n",
    "#hidden layer 2\n",
    "n4_w = np.array([[0.0], [0.0], [0.0], [0.0]])\n",
    "n5_w = np.array([[0.1], [0.1], [0.1], [0.2]])\n",
    "n6_w = np.array([[0.1], [0.1], [0.1], [0.0]])\n",
    "n7_w = np.array([[0.2], [0.2], [0.2], [-0.1]])\n",
    "                 \n",
    "#output layer\n",
    "n8_w = np.array([[1.5], [1.2], [1.0], [0.0], [-0.2]])\n",
    "n9_w = np.array([[0.0], [0.8], [0.1], [0.0], [-0.1]])\n",
    "                 \n",
    "###############################################\n",
    "# Epochs\n",
    "###############################################\n",
    "alpha = 0.1 # learning rate\n",
    "epoch = 1000\n",
    "err_break = 0.001 # stop when below this error\n",
    "err = np.zeros((epoch,1))\n",
    "end_index = epoch-1 # used to tell on what epoch the error passed\n",
    "inds = np.asarray([0,1,2,3,4,5,6,7]) #indexes into the input\n",
    "for k in range(epoch):\n",
    "    \n",
    "    # init error\n",
    "    err[k] = 0    \n",
    "    \n",
    "    # doing online, go through each point, one at a time\n",
    "    for i in range(8): \n",
    "        \n",
    "        # index (which input)\n",
    "        inx = inds[i]\n",
    "        \n",
    "        # forward pass\n",
    "        # hidden layer 1, takes input and outputs to hidden layer 2\n",
    "        oh1 = np.ones((4, 1)) #num neurons +1 for bias term\n",
    "        oh1[0] = np.dot(X[inx,:], n1_w)\n",
    "        oh1[0] = tanh(oh1[0])  \n",
    "        oh1[1] = np.dot(X[inx,:], n2_w)\n",
    "        oh1[1] = tanh(oh1[1]) \n",
    "        oh1[2] = np.dot(X[inx,:], n3_w)\n",
    "        oh1[2] = tanh(oh1[2]) \n",
    "        # hidden layer 2, takes input from hidden layer 1 and outputs to the output layer\n",
    "        oh2 = np.ones((5, 1)) #num neurons +1 for bias term\n",
    "        oh2[0] = np.dot(np.transpose(oh1), n4_w)\n",
    "        oh2[0] = tanh(oh2[0])       \n",
    "        oh2[1] = np.dot(np.transpose(oh1), n5_w)\n",
    "        oh2[1] = tanh(oh2[1]) \n",
    "        oh2[2] = np.dot(np.transpose(oh1), n6_w)\n",
    "        oh2[2] = tanh(oh2[2]) \n",
    "        oh2[3] = np.dot(np.transpose(oh1), n7_w)\n",
    "        oh2[3] = tanh(oh2[3]) \n",
    "        #output layer, takes input from hidden layer 2 and outputs final results, no non-linearity function\n",
    "        ol = np.ones((3, 1)) #num neurons +1 for bias term\n",
    "        ol[0] = np.dot(np.transpose(oh2), n8_w)\n",
    "        ol[1] = np.dot(np.transpose(oh2), n9_w)\n",
    "                \n",
    "        # error\n",
    "        \n",
    "        #error signal output neuron 1\n",
    "        erroro1 = ((1.0/2.0) * np.power(ol[0] - y[inx][0], 2.0))\n",
    "        #error signal output neuron 2\n",
    "        erroro2 = ((1.0/2.0) * np.power(ol[1] - y[inx][1], 2.0))\n",
    "        #instantaneous error for the whole network\n",
    "        err[k] = err[k] + (erroro1 + erroro2)\n",
    "                \n",
    "        # backprop\n",
    "        \n",
    "        # output layer, neuron 1\n",
    "        delta_1 = (-1.0) * (y[inx][0] - ol[0]) #de/o1, since no non-linearity\n",
    "        \n",
    "        # prop it back to the weights\n",
    "        delta_ow1 = np.ones((5, 1))\n",
    "        # delta_index = (input to final neuron) * (Err derivative)\n",
    "        delta_ow1[0] = oh2[0]  *  (delta_1)\n",
    "        delta_ow1[1] = oh2[1]  *  (delta_1)\n",
    "        delta_ow1[2] = oh2[2]  *  (delta_1)\n",
    "        delta_ow1[3] = oh2[3]  *  (delta_1)\n",
    "        delta_ow1[4] = oh2[4]  *  (delta_1)\n",
    "                           \n",
    "        # output layer, neuron 2\n",
    "        delta_12 = (-1.0) * (y[inx][1] - ol[1]) #de/o2, since no non-linearity\n",
    "                   \n",
    "        # prop it back to the weights\n",
    "        delta_ow2 = np.ones((5, 1))\n",
    "        # delta_index = (input to final neuron) * (Err derivative)\n",
    "        delta_ow2[0] = oh2[0]  *  (delta_12)\n",
    "        delta_ow2[1] = oh2[1]  *  (delta_12)\n",
    "        delta_ow2[2] = oh2[2]  *  (delta_12)\n",
    "        delta_ow2[3] = oh2[3]  *  (delta_12)\n",
    "        delta_ow2[4] = oh2[4]  *  (delta_12)\n",
    "         \n",
    "        # hidden layer 2   \n",
    "                \n",
    "        # neuron n4\n",
    "        delta_3 = tanh(oh2[0],derive=True)\n",
    "        #error from the output of neuron n4 (o1 error + o2 error) * tanh der of n4 output\n",
    "        delta_err_out_4 = (delta_3 * ((delta_1 * n8_w[0]) + (delta_12 * n9_w[0])))\n",
    "\n",
    "        # prop back to weights\n",
    "        delta_hw1 = np.ones((4, 1))\n",
    "        #              input      error from output of n4\n",
    "        delta_hw1[0] = oh1[0]  *  delta_err_out_4\n",
    "        delta_hw1[1] = oh1[1]  *  delta_err_out_4\n",
    "        delta_hw1[2] = oh1[2]  *  delta_err_out_4\n",
    "        delta_hw1[3] = oh1[3]  *  delta_err_out_4\n",
    "    \n",
    "        # neuron n5\n",
    "        delta_4 = tanh(oh2[1],derive=True)\n",
    "        #error from the output of neuron n5 (o1 error + o2 error) * tanh der of n5 output\n",
    "        delta_err_out_5 = (delta_4 * ((delta_1 * n8_w[1]) + (delta_12 * n9_w[1])))\n",
    "\n",
    "        # same, need to prop back to weights        \n",
    "        delta_hw2 = np.ones((4, 1))\n",
    "        #              input   error from output of n5\n",
    "        delta_hw2[0] = oh1[0]  *delta_err_out_5\n",
    "        delta_hw2[1] = oh1[1]  *delta_err_out_5\n",
    "        delta_hw2[2] = oh1[2]  *delta_err_out_5\n",
    "        delta_hw2[3] = oh1[3]  *delta_err_out_5\n",
    "            \n",
    "        # neuron n6\n",
    "        delta_5 = tanh(oh2[2],derive=True)\n",
    "        #error from the output of neuron n6 (o1 error + o2 error) * tanh der of n6 output\n",
    "        delta_err_out_6 = (delta_5 * ((delta_1 * n8_w[2]) + (delta_12 * n9_w[2])))\n",
    "        \n",
    "        # same, need to prop back to weights   \n",
    "        delta_hw3 = np.ones((4, 1))\n",
    "        #              input   error from output of n6\n",
    "        delta_hw3[0] = oh1[0]  *delta_err_out_6\n",
    "        delta_hw3[1] = oh1[1]  *delta_err_out_6\n",
    "        delta_hw3[2] = oh1[2]  *delta_err_out_6\n",
    "        delta_hw3[3] = oh1[3]  *delta_err_out_6\n",
    "                  \n",
    "        # neuron n7\n",
    "        delta_6 = tanh(oh2[3],derive=True)\n",
    "        #error from the output of neuron n7 (o1 error + o2 error) * tanh der of n7 output\n",
    "        delta_err_out_7 = (delta_6 * ((delta_1 * n8_w[3]) + (delta_12 * n9_w[3])))\n",
    "        \n",
    "        # same, need to prop back to weights        \n",
    "        delta_hw4 = np.ones((4, 1))\n",
    "        #              input      error from output of n7 \n",
    "        delta_hw4[0] = oh1[0]  *  delta_err_out_7 \n",
    "        delta_hw4[1] = oh1[1]  *  delta_err_out_7 \n",
    "        delta_hw4[2] = oh1[2]  *  delta_err_out_7 \n",
    "        delta_hw4[3] = oh1[3]  *  delta_err_out_7 \n",
    "        \n",
    "        #hidden layer 1\n",
    "        \n",
    "        # neuron n1\n",
    "        delta_7 = tanh(oh1[0],derive=True)\n",
    "        #error from the output of neuron n1 (error based on output to ((n4_err *weight) (n5_err *weight) (n6_err *weight) (n7_err *weight) )* tanh der of n1 output )\n",
    "        delta_err_out_1 = (delta_7 * ((delta_err_out_4*n4_w[0]) + (delta_err_out_5*n5_w[0]) + (delta_err_out_6*n6_w[0]) + (delta_err_out_7*n7_w[0])))\n",
    "        \n",
    "        # same, need to prop back to weights\n",
    "        delta_hw5 = np.ones((4, 1))\n",
    "        # format\n",
    "        #              input     error from output of n1\n",
    "        delta_hw5[0] = X[inx,0]  * delta_err_out_1\n",
    "        delta_hw5[1] = X[inx,1]  * delta_err_out_1\n",
    "        delta_hw5[2] = X[inx,2]  * delta_err_out_1\n",
    "        delta_hw5[3] = X[inx,3]  * delta_err_out_1\n",
    "                   \n",
    "        # neuron n2\n",
    "        delta_8 = tanh(oh1[1],derive=True)\n",
    "        #error from the output of neuron n2 (error based on output to ((n4_err *weight) (n5_err *weight) (n6_err *weight) (n7_err *weight) )* tanh der of n2 output )\n",
    "        delta_err_out_2 = (delta_8 * ((delta_err_out_4*n4_w[1]) + (delta_err_out_5*n5_w[1]) + (delta_err_out_6*n6_w[1]) + (delta_err_out_7*n7_w[1])))\n",
    "        \n",
    "        # same, need to prop back to weights\n",
    "        delta_hw6 = np.ones((4, 1))\n",
    "        # format\n",
    "        #              input     error from output of n2\n",
    "        delta_hw6[0] = X[inx,0]  *  delta_err_out_2\n",
    "        delta_hw6[1] = X[inx,1]  *  delta_err_out_2\n",
    "        delta_hw6[2] = X[inx,2]  *  delta_err_out_2\n",
    "        delta_hw6[3] = X[inx,3]  *  delta_err_out_2\n",
    "                   \n",
    "        # neuron n3\n",
    "        delta_9 = tanh(oh1[2],derive=True)\n",
    "        #error from the output of neuron n3 (error based on output to ((n4_err *weight) (n5_err *weight) (n6_err *weight) (n7_err *weight) )* tanh der of n3 output )\n",
    "        delta_err_out_3 = (delta_9 * ((delta_err_out_4*n4_w[2]) + (delta_err_out_5*n5_w[2]) + (delta_err_out_6*n6_w[2]) + (delta_err_out_7*n7_w[2])))\n",
    "        \n",
    "        # same, need to prop back to weights\n",
    "        delta_hw7 = np.ones((4, 1))\n",
    "        # format\n",
    "        #              input     error from output of n3\n",
    "        delta_hw7[0] = X[inx,0]  *  delta_err_out_3\n",
    "        delta_hw7[1] = X[inx,1]  *  delta_err_out_3\n",
    "        delta_hw7[2] = X[inx,2]  *  delta_err_out_3\n",
    "        delta_hw7[3] = X[inx,3]  *  delta_err_out_3\n",
    "\n",
    "        # update weights\n",
    "        n1_w = n1_w - alpha * delta_hw5 # neuron 1 in hidden layer 1\n",
    "        n2_w = n2_w - alpha * delta_hw6 # neuron 2 in hidden layer 1\n",
    "        n3_w = n3_w - alpha * delta_hw7 # neuron 3 in hidden layer 1\n",
    "        n4_w = n4_w - alpha * delta_hw1 # neuron 1 in hidden layer 2\n",
    "        n5_w = n5_w - alpha * delta_hw2 # neuron 2 in hidden layer 2\n",
    "        n6_w = n6_w - alpha * delta_hw3 # neuron 3 in hidden layer 2\n",
    "        n7_w = n7_w - alpha * delta_hw4 # neuron 4 in hidden layer 2\n",
    "        n8_w = n8_w - alpha * delta_ow1 # neuron 1 in output layer\n",
    "        n9_w = n9_w - alpha * delta_ow2 # neuron 2 in output layer\n",
    "                        \n",
    "    if( err[k] < err_break ):\n",
    "        print(\"error passed on epoch \", k)\n",
    "        end_index = k\n",
    "        break\n",
    "        \n",
    "# plot error    \n",
    "plt.plot(err[0:end_index])\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()\n",
    "        \n",
    "print(\"\\n-----Printing final outputs compared to expected outputs for each test data-----\\n\")\n",
    "    \n",
    "# get values \n",
    "for i in range(8): \n",
    "    \n",
    "    inx = inds[i]\n",
    "    \n",
    "    # forward pass\n",
    "    # hidden layer 1\n",
    "    oh1 = np.ones((4, 1))\n",
    "    oh1[0] = np.dot(X[inx,:], n1_w)\n",
    "    oh1[0] = tanh(oh1[0])  \n",
    "    oh1[1] = np.dot(X[inx,:], n2_w)\n",
    "    oh1[1] = tanh(oh1[1]) \n",
    "    oh1[2] = np.dot(X[inx,:], n3_w)\n",
    "    oh1[2] = tanh(oh1[2]) \n",
    "    # hidden layer 2\n",
    "    oh2 = np.ones((5, 1))\n",
    "    oh2[0] = np.dot(np.transpose(oh1), n4_w)\n",
    "    oh2[0] = tanh(oh2[0])       \n",
    "    oh2[1] = np.dot(np.transpose(oh1), n5_w) \n",
    "    oh2[1] = tanh(oh2[1]) \n",
    "    oh2[2] = np.dot(np.transpose(oh1), n6_w) \n",
    "    oh2[2] = tanh(oh2[2]) \n",
    "    oh2[3] = np.dot(np.transpose(oh1), n7_w) \n",
    "    oh2[3] = tanh(oh2[3]) \n",
    "    #output layer\n",
    "    ol = np.ones((3, 1))\n",
    "    ol[0] = np.dot(np.transpose(oh2), n8_w) \n",
    "    ol[1] = np.dot(np.transpose(oh2), n9_w) \n",
    "        \n",
    "    print(\"Test data \",str(i) + \": produced: \" + str(ol[0]),\" \",str(ol[1]) + \" expected \" + str(y[i][0]), \" \", str(y[i][1]))\n",
    "    \n",
    "print(\"\\n-----Printing final weights and bias - bias is the last term in each array-----\\n\")\n",
    "print(\"Weights and bias for neuron 1 hidden layer 1:\\n\", n1_w)\n",
    "print(\"Weights and bias for neuron 2 hidden layer 1:\\n\", n2_w)\n",
    "print(\"Weights and bias for neuron 3 hidden layer 1:\\n\", n3_w)\n",
    "print(\"Weights and bias for neuron 1 hidden layer 2:\\n\", n4_w)\n",
    "print(\"Weights and bias for neuron 2 hidden layer 2:\\n\", n5_w)\n",
    "print(\"Weights and bias for neuron 3 hidden layer 2:\\n\", n6_w)\n",
    "print(\"Weights and bias for neuron 4 hidden layer 2:\\n\", n7_w)\n",
    "print(\"Weights and bias for neuron 1 output layer:\\n\", n8_w)\n",
    "print(\"Weights and bias for neuron 2 output layer:\\n\", n9_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
